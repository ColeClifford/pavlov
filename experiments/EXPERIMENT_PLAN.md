# Experiment Plan for Pavlov: Rotation-Conditioned Shared Embeddings

*Generated by Atlas | 2026-02-16*

---

## Executive Summary

The core thesis is: **Rotation transforms on a shared embedding matrix can effectively condition multimodal representations while maintaining cross-modal alignment.** The key questions are:
1. Are rotations necessary (vs. simpler transforms)?
2. How important is the shared embedding vs. separate per-modality embeddings?
3. What loss components drive performance?
4. How does this scale with dimensionality and dataset complexity?

---

## **Priority 1: Core Validation Experiments**

### **E1.1: Shared Embedding Ablation**
**Hypothesis:** The shared embedding W is critical for cross-modal alignment; separate per-modality embeddings will fail at cross-modal transfer even with rotation conditioning.

**Setup:**
- Baseline: Current Pavlov (shared W, rotations R_v, R_a)
- Ablation 1: Separate embeddings W_v, W_a (no sharing)
- Ablation 2: Shared W but identity rotations (R_v = R_a = I)
- Ablation 3: Shared W, fixed random rotations (non-learned)

**Metrics:** Cross-modal transfer accuracy (vision→audio, audio→vision), same-modal classification accuracy, cross-modal reconstruction MSE

**Expected Insight:** Quantify the contribution of parameter sharing vs. rotation conditioning. If separate embeddings perform comparably, the shared space isn't necessary. If identity rotations perform well, the rotations are unnecessary complexity.

**Difficulty:** Easy (config changes only)  
**Priority:** **HIGH** - This validates the fundamental architecture choice

---

### **E1.2: Rotation Parameterization Comparison**
**Hypothesis:** Lie algebra SO(d) rotations outperform simpler affine transforms for modality conditioning because they preserve geometry.

**Setup:**
- Baseline: SO(d) via skew-symmetric + matrix_exp (current)
- Variant 1: Learned scaling + bias (z_m = diag(s_m) ⊙ z_shared + b_m)
- Variant 2: Learned orthogonal projection (Householder reflections)
- Variant 3: Learned linear transform (unconstrained W_m matrix, no orthogonality)
- Variant 4: Block-diagonal 2D rotations (RoPE-style)

**Metrics:** Cross-modal transfer, reconstruction quality, rotation matrix condition number, gradient stability

**Expected Insight:** Determine if the geometric constraint (orthogonality) matters or if simple affine conditioning suffices. SO(d) should preserve distances better, improving cross-modal alignment.

**Difficulty:** Medium (requires implementing alternative parameterizations)  
**Priority:** **HIGH** - Validates the core geometric conditioning claim

---

### **E1.3: Loss Component Ablation**
**Hypothesis:** Cross-modal reconstruction is the key driver of shared representations; contrastive loss provides incremental gains.

**Setup:** Systematically disable loss components:
- Same-modal recon only (w_cross=0, w_contrastive=0, w_ortho=0)
- Cross-modal recon only (w_same=0, w_contrastive=0, w_ortho=0)
- Contrastive only (w_same=0, w_cross=0, w_ortho=0)
- Same + Cross (w_contrastive=0, w_ortho=0) ← current default
- All components enabled

**Metrics:** Cross-modal transfer, embedding alignment (CKA, CCA), t-SNE cluster quality

**Expected Insight:** Identify minimum sufficient objective. Hypothesis: cross-modal reconstruction alone should be sufficient; contrastive loss may help at low data regimes.

**Difficulty:** Easy (config changes)  
**Priority:** **HIGH** - Determines what losses actually matter

---

## **Priority 2: Hyperparameter Optimization**

### **E2.1: Latent Dimensionality Sweep**
**Hypothesis:** Performance plateaus above a threshold dimension; very low dimensions hurt due to information bottleneck.

**Setup:** latent_dim ∈ {16, 32, 64, 128, 256, 512}  
Fixed: embed_dim=256

**Metrics:** Cross-modal transfer, reconstruction quality, training time, rotation matrix rank

**Expected Insight:** Identify the minimum viable latent dimension. Hypothesis: 64-128 is sufficient for AV-MNIST (10 classes), while CREMA-D (6 emotions, higher complexity) needs 128-256.

**Difficulty:** Easy (sweep via config)  
**Priority:** **MEDIUM** - Informs efficient deployment

---

### **E2.2: Embedding Dimension Sweep**
**Hypothesis:** Larger embed_dim improves encoder expressiveness but has diminishing returns above a threshold.

**Setup:** embed_dim ∈ {64, 128, 256, 512, 1024}  
Fixed: latent_dim=128

**Metrics:** Same-modal reconstruction, cross-modal transfer, parameter count vs. accuracy tradeoff

**Expected Insight:** Determine optimal encoder capacity. Expect saturation around 256-512.

**Difficulty:** Easy  
**Priority:** **MEDIUM**

---

### **E2.3: Loss Weight Grid Search**
**Hypothesis:** Current weights (same=1.0, cross=1.0, contrastive=0.5, ortho=0.01) may be suboptimal.

**Setup:** Grid search over:
- w_cross ∈ {0.5, 1.0, 2.0}
- w_contrastive ∈ {0.1, 0.5, 1.0, 2.0}
- w_ortho ∈ {0.0, 0.001, 0.01, 0.1}

**Metrics:** Cross-modal transfer, embedding orthogonality (trace(R_v^T R_a)), convergence speed

**Expected Insight:** Find optimal balance. Hypothesis: higher w_contrastive helps CREMA-D (harder alignment task); w_ortho may be unnecessary if rotations naturally diverge.

**Difficulty:** Medium (computational cost: ~36 runs)  
**Priority:** **MEDIUM** - Improves performance but not fundamental understanding

---

## **Priority 3: Architectural Variants**

### **E3.1: Shared vs. Separate Decoder**
**Hypothesis:** Current architecture uses separate decoders (W_dec is shared, but Decoder_v ≠ Decoder_a). Test if a fully shared decoder works.

**Setup:**
- Baseline: Shared W_dec + separate Decoder_v, Decoder_a (current)
- Variant: Shared W_dec + shared Decoder (outputs interleaved vision/audio tokens, demultiplexed by modality)

**Metrics:** Reconstruction quality, parameter efficiency, cross-modal transfer

**Expected Insight:** Determine if modality-specific decoders are necessary or if the rotation conditioning alone provides sufficient modality-specific information.

**Difficulty:** Medium (requires architectural changes)  
**Priority:** **MEDIUM-LOW** - Interesting for theoretical understanding, less critical for performance

---

### **E3.2: Encoder Architecture Variants**
**Hypothesis:** Current CNNs are optimized for AV-MNIST. Test if the approach generalizes to transformer encoders.

**Setup:**
- Baseline: CNN encoders (current)
- Variant 1: Vision Transformer (ViT) patches for vision
- Variant 2: Audio Spectrogram Transformer (AST) for audio
- Variant 3: Shared ViT encoder for both (with modality-specific input projections)

**Metrics:** Cross-modal transfer, computational cost, data efficiency

**Expected Insight:** Validate that rotation conditioning is architecture-agnostic (should work with any encoder producing embed_dim features).

**Difficulty:** Medium-Hard (requires new encoder implementations)  
**Priority:** **MEDIUM-LOW** - Demonstrates generality but not core contribution

---

### **E3.3: Multi-Rotation Hypothesis**
**Hypothesis:** Using multiple rotation subspaces (like multi-head attention) improves representation capacity.

**Setup:**
- Baseline: Single rotation R_m per modality (current)
- Variant: H=4 rotation heads per modality, each rotates embed_dim/H slice independently, then concatenate
- Metric: Cross-modal transfer, embedding rank

**Expected Insight:** Test if the rotation bottleneck can be alleviated by parallel rotation subspaces.

**Difficulty:** Medium (architectural modification)  
**Priority:** **LOW** - Premature optimization before validating single rotation

---

## **Priority 4: Baseline Comparisons**

### **E4.1: CLIP-Style Contrastive Baseline**
**Hypothesis:** Pavlov (reconstruction-based) outperforms or matches CLIP-style (contrastive-only) on cross-modal transfer due to richer training signal.

**Setup:**
- CLIP baseline: Separate encoders Enc_v, Enc_a → L2-normalized embeddings → InfoNCE loss
- Pavlov: Current architecture (shared W + rotations + reconstruction)
- Fair comparison: same embed_dim, same batch size, same training steps

**Metrics:** Cross-modal transfer, missing-modality robustness, data efficiency (performance at 10%, 25%, 50%, 100% data)

**Expected Insight:** Validate that reconstruction provides stronger supervision than contrastive alone, especially in low-data regimes.

**Difficulty:** Medium (implement CLIP baseline)  
**Priority:** **HIGH** - Critical for positioning against dominant paradigm

---

### **E4.2: Separate Encoders Baseline**
**Hypothesis:** Shared embedding with rotation conditioning outperforms separate encoders aligned via losses.

**Setup:**
- Baseline 1: Separate encoders Enc_v → z_v, Enc_a → z_a, aligned via contrastive + cross-reconstruction
- Baseline 2: Late fusion (separate encoders, concatenate embeddings, joint classifier)
- Pavlov: Current architecture

**Metrics:** Cross-modal transfer, parameter count, inference speed

**Expected Insight:** Quantify parameter efficiency vs. performance tradeoff. Pavlov should have fewer parameters (W is shared) but comparable or better transfer.

**Difficulty:** Medium  
**Priority:** **HIGH** - Validates the shared embedding claim

---

### **E4.3: Multimodal VAE Baselines**
**Hypothesis:** Pavlov (deterministic autoencoder with rotation conditioning) matches or outperforms stochastic multimodal VAEs on cross-modal tasks.

**Setup:**
- Baseline: MMVAE (Mixture-of-Experts VAE) with same latent_dim
- Pavlov: Current architecture
- Metric: Cross-modal reconstruction quality, cross-modal transfer, KL divergence (for VAE)

**Expected Insight:** Determine if the VAE's stochastic latent space provides benefits over deterministic rotation-conditioned embeddings.

**Difficulty:** Hard (requires full VAE implementation)  
**Priority:** **MEDIUM** - Important for positioning but not blocking

---

## **Priority 5: Scaling and Robustness**

### **E5.1: Dataset Scaling: AV-MNIST → CREMA-D**
**Hypothesis:** Architecture scales to harder datasets (6-class emotion recognition vs. 10-class digits).

**Setup:** Evaluate on CREMA-D with:
- Current CREMA-D config (embed_dim=256, latent_dim=128)
- Larger variant (embed_dim=512, latent_dim=256)
- Adjusted loss weights (w_contrastive=2.0, per existing cremad.yaml)

**Metrics:** Emotion classification accuracy, cross-modal transfer, per-class confusion matrix

**Expected Insight:** Identify failure modes at higher complexity. Expect lower absolute accuracy but maintained cross-modal transfer gap vs. baselines.

**Difficulty:** Easy (dataset already supported)  
**Priority:** **HIGH** - Demonstrates generalization beyond toy problem

---

### **E5.2: Missing Modality Robustness**
**Hypothesis:** Pavlov gracefully degrades when one modality is missing at test time.

**Setup:**
- Train on paired (vision, audio) with current architecture
- Test scenarios:
  - Vision only (encode with R_v, classify)
  - Audio only (encode with R_a, classify)
  - Both modalities (ensemble or late fusion)

**Metrics:** Classification accuracy in each scenario, relative degradation vs. full model

**Expected Insight:** Quantify robustness. Hypothesis: single-modality performance should be 80-90% of full performance due to shared W.

**Difficulty:** Easy (evaluation script modification)  
**Priority:** **MEDIUM** - Important practical consideration

---

### **E5.3: Rotation Initialization Sensitivity**
**Hypothesis:** Training is robust to rotation initialization (init_rotation_std).

**Setup:** init_rotation_std ∈ {0.001, 0.01, 0.1, 1.0}  
Metric: Final cross-modal transfer, convergence speed, rotation matrix final norms

**Expected Insight:** Identify stable initialization range. Expect robustness across 0.01-0.1 due to gradient flow through matrix_exp.

**Difficulty:** Easy  
**Priority:** **LOW** - Nice-to-know, not critical

---

### **E5.4: Number of Modalities Scaling**
**Hypothesis:** Architecture scales to >2 modalities with rotation matrices forming an orthogonal basis.

**Setup:** (Requires new dataset)
- 3-way multimodal: vision + audio + text (e.g., video captions)
- Architecture: 3 rotations R_v, R_a, R_t with orthogonality regularization

**Metrics:** 3-way cross-modal transfer (all 6 pairwise directions), rotation orthogonality

**Expected Insight:** Test scaling hypothesis. Challenge: finding suitable 3+ modality dataset at MNIST difficulty.

**Difficulty:** **Hard** (requires new dataset and labels)  
**Priority:** **LOW** - Future work, not initial validation

---

## **Priority 6: Theoretical Analysis**

### **E6.1: Rotation Matrix Evolution Analysis**
**Hypothesis:** Rotations R_v and R_a diverge during training (become more orthogonal) as modality-specific structure is captured.

**Setup:**
- Log rotation matrices every epoch
- Compute: trace(R_v^T R_a), Frobenius norm ||R_v - R_a||_F, principal angles

**Metrics:** Evolution over training, correlation with cross-modal transfer accuracy

**Expected Insight:** Understand what the rotations learn. Hypothesis: early training shows similar rotations (exploring shared space); late training shows orthogonal rotations (capturing modality-specific structure).

**Difficulty:** Easy (logging + post-hoc analysis)  
**Priority:** **MEDIUM** - Critical for interpretability

---

### **E6.2: Embedding Space Geometry Analysis**
**Hypothesis:** Shared space z_shared captures semantic structure; rotated spaces z_v, z_a capture modality-specific structure.

**Setup:**
- Compute CCA between z_v and z_a
- t-SNE visualization of z_shared vs. z_v vs. z_a
- Measure intra-class vs. inter-class distances in each space

**Metrics:** CCA correlation, silhouette score, alignment metrics (CKA)

**Expected Insight:** Validate that rotation conditioning partitions semantic (shared) vs. modality-specific (rotated) information.

**Difficulty:** Medium (analysis scripts)  
**Priority:** **MEDIUM** - Important for understanding mechanism

---

### **E6.3: Gradient Flow Analysis**
**Hypothesis:** Matrix exponential in rotation parameterization provides stable gradients vs. direct orthogonal matrix optimization.

**Setup:**
- Log gradient norms through skew-symmetric params vs. direct R optimization (via Cayley transform or SVD projection)
- Track loss landscape curvature

**Metrics:** Gradient magnitude, convergence stability, final performance

**Expected Insight:** Justify Lie algebra parameterization vs. alternatives.

**Difficulty:** Medium (requires alternative implementations)  
**Priority:** **LOW** - Technical validation, not core contribution

---

## **Recommended Experiment Sequence**

### **Phase 1: Core Validation (Week 1-2)**
Run these **first** to validate fundamental assumptions:
1. **E1.1** - Shared embedding ablation → Validates core architecture
2. **E1.3** - Loss component ablation → Identifies necessary objectives  
3. **E4.1** - CLIP baseline → Positions against dominant approach
4. **E5.1** - CREMA-D scaling → Demonstrates beyond toy problem

**Deliverable:** If shared embeddings + rotations + cross-reconstruction outperform baselines on both AV-MNIST and CREMA-D, the core thesis is validated.

---

### **Phase 2: Mechanism Understanding (Week 3-4)**
Once core validation passes, understand *why* it works:
1. **E1.2** - Rotation parameterization comparison → SO(d) necessity
2. **E6.1** - Rotation evolution → What rotations learn
3. **E6.2** - Embedding geometry → Shared vs. modality-specific structure
4. **E2.3** - Loss weight tuning → Optimize performance

**Deliverable:** Theoretical understanding of the rotation mechanism and optimized hyperparameters.

---

### **Phase 3: Optimization & Baselines (Week 5-6)**
Fine-tune and compare comprehensively:
1. **E2.1** - Latent dimension sweep → Efficiency
2. **E4.2** - Separate encoders baseline → Parameter efficiency claim
3. **E5.2** - Missing modality robustness → Practical utility
4. **E4.3** - Multimodal VAE baseline → Positioning vs. generative models

**Deliverable:** Publication-ready benchmark table comparing all baselines.

---

### **Phase 4: Extensions (Future Work)**
After validation, explore generalizations:
1. **E3.2** - Transformer encoders → Architecture agnosticism
2. **E3.3** - Multi-rotation heads → Capacity scaling
3. **E5.4** - >2 modalities → Theoretical limits

**Deliverable:** Roadmap for scaling beyond proof-of-concept.

---

## **Summary: Critical Experiments**

| ID | Experiment | Validates | Priority | Difficulty |
|----|-----------|-----------|----------|-----------|
| **E1.1** | Shared embedding ablation | Core architecture | **HIGH** | Easy |
| **E1.2** | Rotation parameterization | Geometric conditioning | **HIGH** | Medium |
| **E1.3** | Loss component ablation | Training objective | **HIGH** | Easy |
| **E4.1** | CLIP baseline | vs. Contrastive-only | **HIGH** | Medium |
| **E4.2** | Separate encoders baseline | Parameter sharing | **HIGH** | Medium |
| **E5.1** | CREMA-D scaling | Generalization | **HIGH** | Easy |

**Core thesis validation requires:** E1.1, E1.3, E4.1, E5.1 (all HIGH priority, mostly easy difficulty).

**Full understanding requires:** Add E1.2, E6.1, E6.2 (mechanism analysis).

**Publication-ready benchmarks require:** All Priority HIGH + E2.1, E2.3, E4.3, E5.2.

---

## **Experimental Infrastructure Recommendations**

1. **W&B Integration:** Already supported; use for hyperparameter sweeps (E2.x series)
2. **Hydra Multi-Run:** Leverage for ablations (E1.x can be single sweep with overrides)
3. **Checkpoint Strategy:** Save rotation matrices + embeddings at regular intervals for E6.1 analysis
4. **Evaluation Suite:** Extend `pavlov-eval` to include all baselines for fair comparison
5. **Reproducibility:** Fix seeds, log hardware, use deterministic CUDA operations

---

**This experiment plan provides a complete validation roadmap for the Pavlov architecture, from core proof-of-concept (Phase 1) through mechanistic understanding (Phase 2) to publication-ready benchmarks (Phase 3) and future extensions (Phase 4).**
